{"chapter":{"name":"Operating System","explanation":"In studying operating systems, we learn how computers manage hardware and software resources. This includes understanding how processes (programs in execution) are handled, how memory is managed, and how files are organized and accessed. We also explore how the operating system schedules tasks, ensures security, and allows multiple programs to run simultaneously. Essentially, it's about how the core software that runs the computer ensures everything works smoothly and efficiently.","topics":[{"name":"Chapter Overview","explanation":"TODOS: compile all the topics in one story\n\nIn this over view only we gives the conclusion\n","images":[],"subtopics":[]},{"name":"Basic Talks!","explanation":"There are many components in the computer system.\nThat are\n1. monitor\n2. CPU\n3. memory\n4. GPU\n5. Operating System\n","images":[],"subtopics":[{"name":"Monitor","explanation":"The monitor is the display screen that allows users to see the visual output from the computer. It shows everything from the desktop environment to applications and multimedia.","subsubtopics":[]},{"name":"CPU","explanation":"The CPU is the brain of the computer, responsible for executing instructions from programs. It performs calculations and carries out tasks that drive the functionality of the system.\nThe main hardware component of CPU are:\n\nNOTE: core means CPU\n\n1.ALU (Arithmetic Logic Unit):\nThe ALU performs arithmetic and logical operations. It is responsible for executing mathematical calculations (addition, subtraction, multiplication, division) and logical operations (AND, OR, NOT).\n\n2.Control Unit (CU):\nThe Control Unit directs the operation of the processor. It fetches instructions from memory, decodes them, and then executes them by coordinating the actions of the ALU, registers, and other components.\n\n3.Registers:\nRegisters are small, fast storage locations within the CPU used to hold temporary data and instructions that are being processed. Common types of registers include the accumulator, instruction register, and status register.\n\n4.Cache:\nCache is a small, high-speed memory located inside or close to the CPU. It stores frequently accessed data and instructions to speed up processing. There are typically multiple levels of cache (L1, L2, and sometimes L3) with L1 being the fastest and smallest.\n\n5.Clock:\nThe clock generates a steady stream of timing pulses that synchronize the operations of the CPU. The clock speed, measured in GHz, determines how many cycles per second the CPU can execute.\n\n6.Bus Interface:\nThe bus interface manages the communication between the CPU and other components of the computer, such as memory and input/output devices. It includes data buses, address buses, and control buses.\n\n7.Execution Units:\nExecution units are specialized circuits within the CPU that carry out instructions. Modern CPUs have multiple execution units to handle different types of operations, enabling parallel processing and improving performance.\n\n8.Instruction Decoder:\nThe instruction decoder interprets the binary instructions fetched from memory and converts them into control signals that drive the execution units and other components.","subsubtopics":[{"name":"Processor vs CPU","explanation":"CPU: Specifically refers to the main processing unit in a computer that executes general-purpose instructions.\n\nProcessor: A more general term that can refer to any unit capable of processing data.\nIncludes CPUs, GPUs, DSPs, network processors, and other specialized processing units.\n"}]},{"name":"Memory","explanation":"RAM is temporary storage that the CPU uses to store data that is being actively used or processed. It allows for quick access and manipulation of data, enabling faster performance.","subsubtopics":[]},{"name":"GPU (Graphics Processing Unit)","explanation":"It consists of 1000s of smaller and simpler cores compared to more complex cores of CPU.\n\nThis cores works together to process large blocks of the data simultaneously.\nIt has its own dedicated memory (VRAM).","subsubtopics":[]},{"name":"Operating System","explanation":"The OS is the software that manages the hardware and software resources of the computer. It provides a user interface and acts as an intermediary between users and the computer hardware, allowing for the execution of programs and management of files and devices.\n","subsubtopics":[]}]},{"name":"Introduction to Operating Systems","explanation":"Basic function of OS\nTypes of OS\nJOBS, Process, threads\nMulti tasking and threading.\n","images":["http://localhost:4000/uploads/1717845586051.png"],"subtopics":[{"name":"Definition and Functions\n","explanation":"What if there is no OS?\nFunctions of OS.\nWhat we want from OS?\n","subsubtopics":[{"name":"What if there is no OS?","explanation":"1. Bulky and Complex app. (hardware interaction code must ber in app's code)\n2. Resource Exploitation.\n3. No memory Protection"},{"name":"Functions of OS.","explanation":"1. Resource Management.\n2. Abstraction of hardware from software.\n3. Protection of Application data.\n4. Access to Hardware"},{"name":"What we want from OS?","explanation":"1. Maximum CPU usage\n\n2. Less process Starvation \n(Process starvation occurs in a system when a process is unable to make progress because it is not being allocated the resources it needs to execute.)\n\n3. Higher Priority Job Execution\n(when the high priority job comes immediately the lower priority job that is currecnt executing removed and higher will start executing)"}]},{"name":"Types of Operating Systems","explanation":"JOB----> multiple(programs)\nProgram converted to---->process----->Multiple (thread))\n\nSingle Process OS\nBatch Process OS\nMulti Programming OS\nMulti Tasking OS\nMulti Processing OS\nDistributed OS\nReal time OS","subsubtopics":[{"name":"Single process OS\n","explanation":"Only One process executes at a time from the ready queue.\nP2 only start executing when the P1 is completed.\n\neg: MS DOS\nDisadvantages:\n1. CPU usage is not maximized.\n2. Process starvation present\n3. Priority execuation not possible because next process only start executing once the onging process is completed."},{"name":"Batch-processing operating system\n","explanation":"At this time, The computer were large \nand code were written on punch card. \n\nUser prepares his job and then give it to computer operator (person)\nHe will the make the batch of jobs and then the jobs of the batch will execute one by one.\neg: ATLAS\n\nDisadvantage: same here."},{"name":"Multi Programming OS","explanation":"Core: 1\n\nKeeps multiple jobs in memory So that when P1 goes to I/O then P2 will start executing.\n\nAdvantage: 1. increase the CPU usage.\neg:THE"},{"name":"Multi Tasking OS","explanation":"Core : 1\n\nmany job in memory.\ntime quantum concept \n\nHere each job run for particular time constant and then context switch happens.\n\nAdvantage: \n1. Increase CPU usage\n2. No Process starvation \n3. priority execution possible.\n\neg: CTSS"},{"name":"Multi Processing OS","explanation":"All same \nbut core>1\n\neg: windows"},{"name":"Distributed System","explanation":"Many nodes are connected with one OS.\neach node has its own CPU , memory, GPU and all.\n"},{"name":"Real time OS","explanation":"real time error free , computations within tight-time boundaries.\n\nAir traffic control system, ROBOTS etc."}]},{"name":"Terms.","explanation":"1. Context Switching\n2. Program, process, thread\n3. JOB and process\n","subsubtopics":[{"name":"Context switching","explanation":"P1 process stores it's current state in PCB(process control block) and new process start this is context switching at high level.\n\nWe will study this in indepth."},{"name":"Program, process, thread","explanation":".cpp file is program.\nprogram under execution is Process.\nSingle sequence stream within a process is Thread.\n\nBasically, we have to write in code that this is the thread1, this is the thread2.\n\nSee the threading.py in OS directory.\n\nThread E.g.: \n1. Multiple tabs in a browser\n2. Text editor (When you are typing in an editor, spell-checking, formatting of text and saving the text are done concurrently by multiple threads.)\n"},{"name":"JOB and Process","explanation":"NOTE: A JOB has a program or set of programs with any necessary input data and instructions for the computer on how to process that data.\n\nJOB is a STATIC UNIT of work or task that is submitted to be processed, which may include multiple processes when executed.\n\nProcess: A dynamic entity that is actively executing on the CPU.\nmeans JOB----> multiple(programs)\nProgram converted to---->process----->Multiple (thread))"}]},{"name":"Multi tasking v/s Multi Threading (Interview Question)","explanation":"","subsubtopics":[{"name":"Multi-Tasking","explanation":"1. Ability of OS for More than 1 process execution.\n2. Context switching b/w processes.\n3. Isolation and memory protection."},{"name":"Multi threading","explanation":"1. Ability of OS for More than 1 thread execution.\n2. Context switching b/w thread.\n3. No Isolation and memory protection required."},{"name":"Context switching difference.","explanation":"Doesn’t includes switching of memory \naddress space.\nFast switching.\nCPU’s cache state is preserved.\n\n\nIncludes switching of memory address \nspace.\nSlow switching.\nCPU’s cache state is flushed."}]}]},{"name":"Components of OS","explanation":"After starting of computer we only interact with the OS only.\n\n\nUser space, kernel\nDetailed example of How Program execution works.\nFunctions of Kernel.\nTypes of kernel\nSoftware Interrupt and Context switching\nCPU works for user mode and kernel mode.\nInter Process Communication.","images":[],"subtopics":[{"name":"Basic Flow","explanation":"There are 2 part of OS.\n1. User Space (GUI and CLI)\n2. Kernel\n\nUser space:\nWe can only interact with user space using Graphic user Interface(Screen visuals) and Command line interface.\n\nKernel:\nUser space interact with kernel.\nkernel interact with hardware.\n","subsubtopics":[]},{"name":"Detailed example of How Program execution works.","explanation":"Both user mode and kernel mode tasks are executed by the CPU. In user mode, the CPU performs operations within the allocated memory space of a process, without direct access to hardware or critical system resources. In kernel mode, the CPU performs operations that require direct access to both RAM and storage devices (like SSDs), as directed by the operating system.\n","subsubtopics":[{"name":"User Action:\n","explanation":"Click Run: You click on an executable file or run a command in the terminal to start a program."},{"name":"Operating System Interaction:\n","explanation":"Request to Kernel: The operating system's user interface sends a request to the kernel to load and execute the program."},{"name":"Loading the Program:","explanation":"File System Access: The kernel accesses the file system to locate the executable file on the storage device (e.g., SSD or HDD).\n\nLoading into RAM: The kernel reads the program's executable file from storage and loads it into RAM (random-access memory). This involves copying the program's code, data, and any required libraries into memory."},{"name":"Process Creation:","explanation":"Creating a Process: The kernel creates a process control block (PCB) for the new program. The PCB contains information about the process, such as its ID, program counter, registers, memory pointers, and status.\n\nMemory Allocation: The kernel allocates the necessary memory space for the process, including its stack, heap, and data segments.\n\nSee Introduciton to Process to learn this step in slight depth"},{"name":"CPU Scheduling:","explanation":"Scheduler Activation: The kernel's scheduler determines when the new process will run. If the CPU is currently busy, the process is placed in a ready queue.\n\nContext Switching: When the CPU is ready to execute the new process, the kernel performs a context switch, saving the state of the current process (if any) and loading the state of the new process from its PCB."},{"name":"Executing the Program:","explanation":"Instruction Fetch: The CPU fetches the first instruction of the process from memory.\n\nInstruction Decode: The CPU decodes the instruction to determine what action is required.\n\nInstruction Execute: The CPU executes the instruction, which may involve arithmetic operations, data movement, or I/O operations."},{"name":"Memory Management:","explanation":"Virtual Memory: The kernel uses virtual memory techniques to manage the process's memory. It maps virtual addresses used by the process to physical addresses in RAM.\n\nPage Fault Handling: If the process accesses a part of memory that is not currently in RAM (but stored on disk), the kernel handles the page fault by loading the required page from disk into RAM."},{"name":" I/O Operations:","explanation":"System Calls: If the process needs to perform I/O operations (e.g., read/write files, network communication), it makes system calls to the kernel.\n\nDevice Drivers: The kernel interacts with device drivers to perform the actual I/O operations. For example, reading a file involves sending commands to the storage device's driver."},{"name":"Multitasking:","explanation":"Time-Slicing: The kernel's scheduler periodically interrupts the CPU to perform context switches, allowing multiple processes to share the CPU time (time-slicing).\n\nPriority Scheduling: The scheduler may prioritize certain processes over others based on their priority levels, ensuring important tasks receive adequate CPU time."},{"name":"Termination:","explanation":"Process Completion: When the process completes its execution, it makes a system call to the kernel to terminate.\n\nResource Deallocation: The kernel deallocates the memory and resources used by the process and updates the process table to mark it as terminated."}]},{"name":"How folder is created.","explanation":"","subsubtopics":[{"name":"User Action:","explanation":"Click Create Folder: You click on the option to create a new folder in a file manager (like Windows Explorer, macOS Finder, or a Linux file manager)."},{"name":"Application Request:","explanation":"File Manager: The file manager application sends a request to the operating system to create a new folder."},{"name":" System Call to Kernel:","explanation":"System Call: The file manager makes a system call to the kernel. A system call is a way for user-space applications to request services from the kernel. In this case, it might be a call like mkdir() (make directory)."},{"name":"Kernel Processing:","explanation":"Kernel Receives Request: The kernel receives the mkdir() system call.\n\nCheck Permissions: The kernel checks whether the user has the necessary permissions to create a folder in the specified directory.\n\nFile System Management: The kernel interacts with the file system to create the new folder. This involves:\n\nUpdating File System Metadata: The kernel updates the file system's metadata to include the new folder. This could involve updating directory structures, allocation tables, or inodes, depending on the file system type (e.g., NTFS, ext4, APFS).\n\nAllocating Space: The kernel allocates space for the new directory entry in the storage device (HDD, SSD)."},{"name":"Hardware Interaction:","explanation":"Storage Device: The kernel sends commands to the storage device driver to write the new directory information to the disk. This involves:\n\n  Driver Communication: The kernel communicates with the appropriate device driver for the storage device.\n\n  Physical Write: The storage device writes the necessary data to create the new folder on the disk."},{"name":"Notification to Application:","explanation":"System Call Return: Once the folder is successfully created, the kernel returns a status code to the file manager application, indicating success or failure."},{"name":"User Interface Update:","explanation":"Refresh Display: The file manager updates the user interface to show the newly created folder. This might involve refreshing the view of the current directory to include the new folder."}]},{"name":"Functions of Kernel.","explanation":"Process\tmanagement\nMemory management\nFile management\nI/O management","subsubtopics":[{"name":"Process\tmanagement:","explanation":"1. process creation and termination.\n2. Process and thread scheduling.\n3. Process communication.\n4. Process syncronization."},{"name":"Memory management:","explanation":"Allocation and De-allocation of memory.\nTrack of free and occupied memory."},{"name":"File management:","explanation":"Create and delete file\nDirectory management"},{"name":"I/O management:","explanation":"Management and control all I/O device.\neg: Pendrive.\n\nBuffering , caching, spooling"}]},{"name":"Types of Kernel.","explanation":"Monolithic Kernel\nMicro Kernel\nHybrid Kernel\nNano/Exo kernel","subsubtopics":[{"name":"Monolithic Kernel","explanation":"a. All functions are in kernel.\nb. bulky in size as all functions in kernel.\nc. Memory required to run is high.\nd. Less\treliable, one module crashes -> whole kernel is down.\ne. High\tperformance as communication is\tfast. (Less\tuser mode, kernel mode overheads)\nf. Eg. Linux, Unix, MS-DOS."},{"name":"Micro Kernel","explanation":"a. Only\tmajor functions are in kernel.\ni. Memory mgmt.\nii. Process mgmt.\nb. File\tmgmt. and IO mgmt. are in User-space.\nc. smaller in size.\nd. More\tReliable\ne. More\tstable\nf. Performance is slow.\ng. Overhead switching b/w user mode and kernel mode.\nh. Eg. L4 Linux, Symbian OS, MINIX etc"},{"name":"Hybrid Kernel","explanation":"a. Advantages of both worlds. (File mgmt. in User space\tand rest in Kernel space)\nb. Combined approach.\nc. Speed and design of mono.\nd. Modularity and stability of micro.\ne. Eg.\tMacOS, Windows\tNT/7/10\nf. IPC also happens but\tlesser overheads"},{"name":"Nano/Exo kernels","explanation":""}]},{"name":"Terms","explanation":"Software Interrupt and Context switching\nCPU works for user mode and kernel mode.\nInter Process Communication.","subsubtopics":[{"name":"Software Interrupt and Context Switching","explanation":"Software Interrupt:\n\nTriggered by Program: A software interrupt is usually triggered by a program running in user mode, which requests a service from the operating system.\nTransition to Kernel Mode: The interrupt causes a switch from user mode to kernel mode, where the operating system has higher privileges.\nService Execution: The kernel performs the requested service on behalf of the program.\nReturn to User Mode: After the service is completed, control is returned to the program, which continues execution in user mode.\n\nContext Switch:\n\nManaged by OS Scheduler: Context switches are managed by the operating system's scheduler, which decides when to switch from one process to another based on scheduling policies.\nInterrupted Process: The currently running process is interrupted, and its state (registers, program counter, etc.) is saved.\nSwitch to Another Process: The scheduler selects a new process to run and loads its state into the CPU.\nResuming Execution: The new process resumes execution, and the CPU continues its work on the new process."},{"name":"CPU in user and kernel mode.","explanation":"User Mode: \nThe CPU executes tasks requested by the user through applications. Examples include running applications (like a text editor or web browser), performing calculations (using a spreadsheet or calculator), and rendering graphics (in games or graphic design software). These tasks involve logical operations and data manipulation within the applications and are executed in a restricted environment to ensure system stability and security.\n\nKernel Mode: \nThe CPU performs tasks that involve managing system resources and interacting with hardware. Examples include handling system calls (when applications request operating system services), managing hardware and I/O operations (like reading from or writing to a disk), memory management (allocating memory for applications), process scheduling (managing multiple running processes), handling interrupts (responding to keyboard or mouse inputs), and running device drivers (software that manages hardware devices). These tasks require full access to system resources and are executed in a privileged environment to maintain system control and integrity."},{"name":"Inter Process Communication.","explanation":"1. Two\tprocesses executing independently, having independent memory space (Memory protection), But some\tmay\tneed\tto\tcommunicate\tto\twork.\n2. Done by shared memory and message passing"}]}]},{"name":"System Calls","explanation":"Using System Calls only User mode connects with kernel mode.\n\nSoftware interrupt is also a system call which tells the CPU to change to kernel mode from User mode or vise versa.\n\nuse tasklist in cmd to get all the running process\n\nDefination\nTypes of System calls \n","images":["http://localhost:4000/uploads/1717882097447.png","http://localhost:4000/uploads/1717882106860.png"],"subtopics":[{"name":"Defination","explanation":"A system call is a mechanism using which a user program can request a service from the kernel for which it does not have the permission to perform.\nUser programs typically do not have permission to perform operations like accessing I/O devices and\ncommunicating other programs","subsubtopics":[]},{"name":"Types of system calls","explanation":"Process Control\nFile Management\nDevice Management\nInformation maintenance\nCommunication Management","subsubtopics":[{"name":"Process Control\n","explanation":"a. end, abort\nb. load, execute\nc. create process, terminate process\nd. get process attributes, set process attributes\ne. wait for time\nf. wait event, signal event\ng. allocate and free memory"},{"name":"File Management","explanation":"a. create file, delete file\nb. open, close\nc. read, write, reposition\nd. get file attributes, set file attributes"},{"name":"Device Management","explanation":"a. request device, release device\nb. read, write, reposition\nc. get device attributes, set device attributes\nd. logically attach or detach devices"},{"name":"Information maintenance","explanation":"a. get time or date, set time or date\nb. get system data, set system data\nc. get process, file, or device attributes\nd. set process, file, or device attributes"},{"name":"Communication Management","explanation":"a. create, delete communication connection\nb. send, receive messages\nc. transfer status information\nd. attach or detach remote devices "}]}]},{"name":"Summary Till now.","explanation":"OS has 2 software : user mode and kernel mode.\nEvery thing the any mode want to do is done with the help of CPU.\nAt a time CPU can only work for any one mode. \nTo change the mode System call (software interrupt) has to be called.","images":[],"subtopics":[]},{"name":"What happens when you turn on your computer?","explanation":"PC on \nCPU initializes\nBIOS run\nbios handoff to bootloader\nbootloader execution","images":[],"subtopics":[{"name":"PC On","explanation":"Power ON-> Power supply circuity -> All hardware components get power.","subsubtopics":[]},{"name":"ii. CPU Initializes\n","explanation":"1. The CPU begins its initialization process and looks for a firmware program.\n\n2. This firmware program is stored in a ROM chip on the motherboard called the BIOS (Basic Input-Output System) chip.\nBIOS is non-volatile chip.\n\n3. The BIOS allows the computer to access and set up the system at the most basic level.\n\n4. In modern PCs, the CPU loads the UEFI (Unified Extensible Firmware Interface) instead of the traditional BIOS.","subsubtopics":[]},{"name":"iii. BIOS Runs\n","explanation":"BIOS/UEFI Execution: The CPU runs the BIOS/UEFI, which performs tests and initializes system hardware.","subsubtopics":[{"name":"POST","explanation":"This process is called POST (Power-On Self-Test).\n\nHardware Check: The BIOS/UEFI checks for hardware issues, such as missing RAM.\nAlso loads some settings from a memory area which is charged by CMOS battery.\n\nError Handling: If an issue is detected, an error is thrown, and the boot process is stopped."},{"name":"Advanced Features","explanation":"UEFI offers more functionalities than just hardware initialization; it can act like a tiny operating system.\n\nIntel Management Engine: \nFor example, Intel CPUs have the Intel Management Engine, which powers features like Intel’s Active Management Technology, allowing for remote management of business PCs."}]},{"name":"iv. BIOS Handoff to Bootloader\n","explanation":"Bootloader has instructions to ON OS. \nIt Bootloader is present in MBR in desk. It is the 0th index of desk.\nAt present, Bootloader is present in EFI in desk. It is a partition in disk.","subsubtopics":[{"name":"Bootloader Handoff","explanation":"After initialization, the BIOS/UEFI hands off the responsibility for booting the computer to the operating system's bootloader.\n\nMBR: Traditional BIOS looks at the Master Boot Record (MBR), a special boot sector at the beginning of a disk.\n\nMBR Function: The MBR contains code that loads the rest of the operating system, known as the bootloader.\n"},{"name":"Bootloader Execution:","explanation":"The BIOS executes the bootloader, which then takes over the boot process."}]},{"name":"v. Bootloader Execution\n","explanation":"Bootloader Role: The bootloader is a small program with the critical task of booting the rest of the operating system.","subsubtopics":[{"name":"Loading the Kernel:\n","explanation":"Kernel Location: The bootloader locates the operating system kernel, which is usually stored on the hard drive or SSD.\n\nKernel Loading: It loads the kernel image into RAM (Random Access Memory).\n\nKernel Initialization: Once in RAM, the kernel initializes the system hardware and sets up essential structures needed for the system's operation."},{"name":"Loading Initial User Space:\n","explanation":"Initial Programs: After the kernel is initialized, the bootloader or the kernel itself loads the initial user space programs. These programs include system daemons and essential services required for the operating system to function.\n\nUser Space Setup: These processes are responsible for handling user-level tasks and providing an environment where user applications can run"}]},{"name":"Finally!","explanation":"And then we see HP logo and then password and all","subsubtopics":[]}]},{"name":"Types of Storages in computer","explanation":"Registers\n2. Cache Memory\n3. Main Memory (RAM)\n4. Secondary Storage\n5. Tertiary Storage\n6. Quaternary Storage\n\n","images":["http://localhost:4000/uploads/1718005432751.png","http://localhost:4000/uploads/1718083227358.png"],"subtopics":[{"name":"Registers","explanation":"Primary storage\nSmall, fast storage locations within the CPU used to hold data temporarily during processing.\nTypically a few bytes (32-bit, 64-bit)\nNanoseconds (ns) or less, usually a single CPU cycle\n\n\nNote :- \n1. Each core has its own register.\n2. There are many different type register in a core but each are 1 in quantity.","subsubtopics":[{"name":"Types of register","explanation":"each core has this registers in 1 quantity.\nGeneral-Purpose Registers (GPRs)\nSpecial-Purpose Registers\nFloating-Point Registers\nControl Registers\nSegment Registers (in x86 Architecture)\nVector Registers"},{"name":"What is architecture?","explanation":"in simple terms, the architecture of a processor refers to its design or blueprint. It includes the instruction set, which defines the set of operations that the processor can perform, as well as other features such as the number and types of registers, cache sizes, and overall organization of the CPU cores. The architecture determines how the processor executes instructions and processes data, which ultimately affects its performance, power consumption, and compatibility with software."}]},{"name":"Cache Memory","explanation":"Primary storage\nHigh-speed memory located between the CPU and the main memory to store frequently accessed data and instructions.\n\nL1 Cache: Closest to the CPU core, smallest, and fastest.\nL2 Cache: Larger and slightly slower than L1.\nL3 Cache: Shared among multiple CPU cores, larger, and slower than L2.\n\n\nL1: 32KB to 128KB per core\nL2: 256KB to 1MB per core\nL3: 2MB to 32MB shared\n\nL1: 1-2 ns\nL2: 3-10 ns\nL3: 10-30 ns\n\nEach core has L1 cache but L2 and L3 are shared among cores","subsubtopics":[]},{"name":"Main Memory (RAM)","explanation":"Type: Primary storage\nDescription: Volatile memory used to store data and instructions that the CPU needs while performing tasks.\nDRAM (Dynamic RAM): Common type of main memory.\nSRAM (Static RAM): Faster but more expensive than DRAM, used in cache.\nCapacity: 4GB to 128GB or more\nAccess Time: 50-100 ns (DRAM)","subsubtopics":[]},{"name":"Secondary Storage","explanation":"Type: Secondary storage\nDescription: Non-volatile memory used to store data and programs not currently in use. Includes hard drives, SSDs, and external storage devices.\nHard Disk Drive (HDD): Magnetic storage device.\nSolid-State Drive (SSD): Flash memory storage device.\nCapacity:\nHDD: 500GB to 10TB or more\nSSD: 128GB to 4TB or more\nAccess Time:\nHDD: 5-10 milliseconds (ms)\nSSD: 0.1-1 ms","subsubtopics":[]},{"name":"Tertiary Storage","explanation":"Type: Tertiary storage\nDescription: Storage used for archival and backup purposes, such as tape drives and optical disks.\nOptical Disks: CDs, DVDs, Blu-ray disks.\nTape Drives: Magnetic tape storage.\nCapacity:\nOptical Disks: 700MB (CD), 4.7GB (DVD), 25GB to 100GB (Blu-ray)\nTape Drives: Up to several terabytes\nAccess Time:\nOptical Disks: 100 milliseconds (ms) or more\nTape Drives: 10 seconds or more (due to sequential access)","subsubtopics":[]},{"name":"Quaternary Storage","explanation":"Type: Quaternary storage\nDescription: Remote storage accessed over a network, such as cloud storage.\nCloud Storage: Data stored in remote servers accessed via the internet.\nCapacity: Virtually unlimited, depending on the service provider\nAccess Time: Variable, depending on network speed and latency, typically in the range of milliseconds to seconds\n","subsubtopics":[]}]},{"name":"32 bit v/s 64 bit OS","explanation":"OS is simply software.\nIt is made based on register. \nIf the register is 64 bit the 32 bit or 64 bit any OS can be used.\nBut if the register is 32 then only 32 bit OS is used.\n\n\ndetails \nadvantages\n","images":[],"subtopics":[{"name":"Details","explanation":"1. A 32-bit OS has 32-bit registers, and it can access 2^32 unique memory addresses. i.e., 4GB of\nphysical memory.\n2. A 64-bit OS has 64-bit registers, and it can access 2^64 unique memory addresses. i.e.,\n17,179,869,184 GB of physical memory.\n3. 32-bit CPU architecture can process 32 bits of data & information.\n4. 64-bit CPU architecture can process 64 bits of data & information","subsubtopics":[]},{"name":"Advantage of 64 bit over 32 bit","explanation":"a. Addressable Memory: \n32-bit CPU -> 2^32 memory addresses, 64-bit CPU -> 2^64 memory addresses.\n\nb. Resource usage: \nInstalling more RAM on a system with a 32-bit OS doesn't impact\nperformance. However, upgrade that system with excess RAM to the 64-bit version of Windows, and you'll notice a difference.\n\nc. Performance: \nAll calculations take place in the registers. When you’re performing math in your code, operands are loaded from memory into registers. So, having larger registers allow you to perform larger calculations at the same time. 32-bit processor can execute 4 bytes of data in 1 instruction cycle while 64-bit means that processor can execute 8 bytes of data in 1 instruction cycle. (In 1 sec, there could be thousands to billons of instruction cycles depending upon a\nprocessor design)\n\nd. Compatibility: \n64-bit CPU can run both 32-bit and 64-bit OS. While 32-bit CPU can only run 32-bit OS.\n\ne. Better Graphics performance: \n8-bytes graphics calculations make graphics-intensive apps\nrun faster.","subsubtopics":[]}]},{"name":"Introduction To Process.","explanation":"Steps of process creation \nPCB details\nprocess states\nProcess Queues\nDispatcher and LTS & STS\nstory\n","images":["http://localhost:4000/uploads/1718084499829.png","http://localhost:4000/uploads/1718089632172.png"],"subtopics":[{"name":"Steps of process creation.","explanation":"a. Load the program & static data into memory.\nb. Allocate runtime stack.\nc. Heap memory allocation.\nd. IO tasks.\ne. OS handoffs control to main ().","subsubtopics":[{"name":"a. Load the program & static data into memory.\n","explanation":"Loading the Program: When a new process is created, the operating system (OS) loads the program's executable code from storage (like a hard drive or SSD) into the main memory (RAM). This includes the instructions that the CPU will execute.\n\nStatic Data: This also involves loading static data sections, which include global variables and constants defined in the program. These are typically stored in specific sections of memory."},{"name":"b. Allocate runtime stack.\n","explanation":"Runtime Stack Allocation: The OS allocates a runtime stack for the process. The stack is used for managing function calls, local variables, and control flow within the program. Each time a function is called, a new stack frame is created on the top of the stack to store the function's parameters, return address, and local variables."},{"name":"c. Heap memory allocation.\n","explanation":"Heap Allocation: The OS allocates a region of memory known as the heap for dynamic memory allocation. The heap is used for allocating memory at runtime using functions like malloc in C or new in C++. Unlike the stack, which operates in a last-in, first-out manner, the heap allows for flexible memory allocation and deallocation."},{"name":"d. IO tasks.\n","explanation":"Input/Output Tasks: The OS sets up the necessary resources and permissions for the process to perform input/output operations. This may include opening files, establishing network connections, or setting up communication channels with other processes. The OS ensures that the process has access to the required IO devices and that they are managed correctly."},{"name":"e. OS handoffs control to main ().\n\n","explanation":"Control Transfer: After setting up the program in memory, allocating the stack and heap, and preparing IO resources, the OS hands control over to the process's main() function (or the entry point of the program). This means that the CPU starts executing the instructions in the main() function, and the process begins its execution."}]},{"name":"PCB (Process control Block)","explanation":"PCB stores all the attributes of the process that are important for resume of process.\n\nThis are the some attributes.\n\nProcess id : Unique for every process.\nParent Process ID (PPID):\nProgram counter : It tells on which line the process reached.\nProcess State : New, wait, run etc\nPriority : Based on priority a process gets CPU time.\nRegister : Save the value in the CPU register here. So that when process resumes then use it.\nList of open files\nList of open devices","subsubtopics":[{"name":"All Attributes","explanation":"The Process Control Block (PCB) is a critical data structure used by the operating system to manage processes. The PCB contains all the information about a specific process, including its current state, resources allocated to it, and other essential details. Here's a detailed breakdown of the typical structure of a PCB:\n\n1. Process Identification Information\n\nProcess ID (PID): A unique identifier for the process.\nParent Process ID (PPID): The PID of the parent process that created this process.\n\nUser ID (UID) and Group ID (GID): Identifiers for the user and group owning the process.\n\n2. Process State Information\n\nProcess State: The current state of the process (e.g., running, waiting, ready, terminated).\n\nProgram Counter (PC): The address of the next instruction to be executed.\n\nCPU Registers: The contents of all the CPU registers for the process.\n\nStack Pointer: Points to the top of the current stack in memory.\n\nBase Pointer: Points to the base of the stack.\n\nStatus Register: Contains flags that indicate the status of the CPU (e.g., zero flag, carry flag, sign flag).\n\n3. CPU Scheduling Information\n\nPriority: The priority level of the process.\n\nScheduling Queue Pointers: Pointers to other PCBs in the scheduling queue.\n\nScheduling Information: Other scheduling parameters like quantum (time slice for the process).\n\n4. Memory Management Information\n\nBase and Limit Registers: Define the address space allocated to the process.\n\nPage Tables or Segment Tables: Used for memory management in systems with paging or segmentation.\n\nHeap Information: Details about the heap segment for dynamic memory allocation.\n\n5. Accounting Information\n\nCPU Usage: Total CPU time used by the process.\nUser and System Time: The amount of time the process has spent in user mode and system mode.\n\nCreation Time: The time when the process was created.\n\nTermination Time: The time when the process terminated.\n\nExecution Time: The total execution time of the process.\n\n6. I/O Status Information\n\nOpen File Descriptors: A list of files currently opened by the process.\n\nI/O Devices: Information about I/O devices allocated to the process.\n\nI/O Requests: Details about pending I/O operations.\n\n7. Inter-process Communication (IPC) Information\n\nIPC Mechanisms: Information about IPC mechanisms used by the process, such as message queues, shared memory, and semaphores.\n\nCommunication Partners: Identifiers for other processes that the process is communicating with.\n\n8. Miscellaneous Information\n\nProcess Signals: Information about signals sent to the process and signal handlers.\n\nEnvironment Variables: Environment settings for the process.\n\nResource Usage: Information about other resources used by the process, such as network connections.\n"}]},{"name":"Process states","explanation":"From generation to termination\n\nNew \nReady \nRunning \nWaiting\nTerminated","subsubtopics":[{"name":"New state","explanation":"For process creation, Kernel make PCB first then It will do process creation steps.\n\nfrom PCB making complete to completing all steps the state of the process is new.\n\nWhen the process has new state It will be present in JOB QUEUE.\n"},{"name":"Ready state","explanation":"Once the process is created then till running starts.\nState is ready. \n\nWhen the state is ready it will present in READY QUEUE."},{"name":"Running","explanation":"Process is executing"},{"name":"waiting","explanation":"When the process is doing I/O then the state is called waiting state. \nBecause it is waiting for an external event (such as data transfer to/from an I/O device) to complete.\n\neg:The process is executing (running state) and requests an I/O operation (e.g., reading a file).\nThe process makes a system call to initiate the I/O operation.\nThe OS moves the process to the waiting state and adds it to the I/O wait queue."},{"name":"Terminated.","explanation":"The process has finished execution. PCB entry removed from process table. \nBut PCB exists."}]},{"name":"Process Queues","explanation":"a. Job Queue:\n\ni. Processes in new state.\nii. Present in secondary memory.\niii. Job Schedular (Long term schedular (LTS)) picks process from the pool and loads them into memory for execution.\n\nb. Ready Queue:\n\ni. Processes in Ready state.\nii. Present in main memory.\niii. CPU Schedular (Short-term schedular) picks process from ready queue and dispatch it to CPU.\n\nc. Waiting Queue:\n\ni. Processes in Wait state.","subsubtopics":[]},{"name":"Dispatcher and LTS & STS","explanation":"The OS module that gives control of the CPU to a process selected by the short-term scheduler (STS) is called the dispatcher.\n\nDegree of Multiprogramming:\n\nThis refers to the number of processes in memory at any given time.\nThe long-term scheduler (LTS) controls the degree of multiprogramming.\nThe long-term scheduler is called \"long-term\" because it runs or checks processes at relatively long intervals (not 1 second, but longer compared to the short-term scheduler). The short-term scheduler runs or checks processes very frequently, often every millisecond or even less.\n\nLogically, the LTS is responsible for controlling the degree of multiprogramming by deciding which processes should be brought into memory.","subsubtopics":[]},{"name":"Full process story","explanation":"Initially, a process is in the \"new\" state once the PCB is created for it. At this point, the kernel is creating the process, and it is present in the job queue.\n\nOnce the process creation is complete, the long-term scheduler (job scheduler) moves it to the ready queue and changes the state of the process to \"ready.\"\n\nThe short-term scheduler (CPU scheduler) then selects the process and dispatches it to the CPU. The dispatcher gives control of the CPU to that process, and its state changes to \"running.\"\n\nThere are three scenarios for the running process:\n\nTime Slice Ends: If the allotted time ends, a context switch occurs. The CPU selects a new process, and the old process is sent back to the ready queue with its state changed to \"ready.\"\n\nProcess Completion: If the process completes its execution, its state changes to \"terminated.\"\n\nI/O Operation: If the process needs to perform an I/O operation, its state changes to \"waiting.\" Once the I/O operation is complete, the process will be moved back to the ready queue with its state changed back to \"ready.\"","subsubtopics":[]}]},{"name":"Terminologies ","explanation":"Need of Mid term Schedular\nContext switching\nOrphan Process\nZombie Process","images":[],"subtopics":[{"name":"Swaping","explanation":"MTS will swap some process out and store them in the swap space (secondary storage).\nWhenever process needs to execute then MTS will swap them in.\n\nNeed of MTS : \nHandling Limited Physical Memory:\n\nComputers often have a limited amount of RAM. When running multiple applications or processes simultaneously, the total memory required can exceed the available RAM.\nSwapping allows the operating system to move inactive or less frequently used processes from RAM to a swap space (usually on a hard drive or SSD), freeing up RAM for active processes.\nSupporting Multitasking:\n\nModern operating systems support multitasking, which means running multiple processes at the same time. Without swapping, the system might not be able to handle more processes than can fit into the available RAM.\nSwapping ensures that all processes get a chance to execute by temporarily moving some processes out of RAM.","subsubtopics":[]},{"name":"Context Switching","explanation":"In context switching ,the kernel saves the context of the old process in its PCB and loads the saved context of the new process scheduled to run.\n\nNow this is pure overhead because during this time CPU is not working.\n\nWe have to reduce this overhead.\nWhich will be reduced by register performance and memory speed.","subsubtopics":[]},{"name":"Orphan process\n","explanation":"a. The process whose parent process has been terminated and it is still running.\nb. Orphan processes are adopted by init process.\nc. Init is the first process of OS.","subsubtopics":[{"name":"Hands on example ","explanation":"in terminal run \n\ntasklist \nto see all the process\n\nwmic process get ProcessId,ParentProcessId,Name\nto get the process with their pid and ppid \n\n\nNow to see the example just go to OS directory and run parent.ps1 in the power shell\nuseing \n.\\parent.ps1\n\nthen to verify\nGet-Process -Name powershell\n"}]},{"name":"Zombie Process","explanation":"a. A zombie process is a process whose execution is completed but it still has an entry in the process\ntable.\nb. Zombie processes usually occur for child processes, as the parent process still needs to read its\nchild’s exit status. Once this is done using the wait system call, the zombie process is eliminated\nfrom the process table. This is known as reaping the zombie process.\nc. It is because parent process maycall wait () on child process for a longer time duration and child\nprocess got terminated much earlier.\nd. As entry in the process table can only be removed, after the parent process reads the exit status of\nchild process. Hence, the child process remains a zombie till it is removed from the process table","subsubtopics":[{"name":"Hands on example","explanation":"To simulate a \"zombie\" process in Windows PowerShell, we can create a scenario where a process is started and then blocked indefinitely, causing it to become unresponsive. Here's an example:\n\n1. Create a PowerShell script named `zombie.ps1` with the following content:\n\n   ```powershell\n   # Display a message and then enter an infinite loop\n   Write-Host \"Zombie process started\"\n   while ($true) {\n       Start-Sleep -Seconds 1\n   }\n   ```\n\n2. Run the `zombie.ps1` script. It will start and enter an infinite loop, effectively becoming unresponsive.\n\n3. In another PowerShell window, run the following command to list all processes and their statuses:\n\n   ```powershell\n   Get-Process | Format-Table -Property Id, Name, Responding\n   ```\n\n   Look for the process named `powershell.exe` (or `zombie.ps1` depending on the version of PowerShell) and check its `Responding` status. It should be `False`, indicating that the process is not responsive.\n\n4. You can end the \"zombie\" process by terminating its parent process or by using the `Stop-Process` cmdlet to forcibly stop it:\n\n   ```powershell\n   Stop-Process -Name powershell\n   ```\n\n   Replace `powershell` with the actual process name if it's different. This will end the \"zombie\" process and remove it from the list of processes."}]}]},{"name":"CPU scheduling Algos. ","explanation":"Algo which is used to select the job from ready queue and dispatch.\nSTS runs this algorithm.\n\npreemptive and non-preemptive\ndifferent times\nGoals\nFCFS\nSJF\nPriority Scheduling\nRound robin scheduling\nMulti-level queue scheduling\nMulti-level feedback queue scheduling\nconvoy effect and process starvation","images":["http://localhost:4000/uploads/1718111540923.png","http://localhost:4000/uploads/1718112209294.png","http://localhost:4000/uploads/1718112449281.png"],"subtopics":[{"name":" Non-Preemptive and preemptive scheduling ","explanation":"Non-Preemptive scheduling\n\na. Once CPU has been allocated to a process, the process keeps the CPU until it releases CPU either by terminating or by switching to wait-state.\nb. Starvation, as a process with long burst time may starve less burst time process.\nc. Low CPU utilization.\n\nPreemptive scheduling\n\na. CPU is taken away from a process after time quantum expires along with terminating or switching\nto wait-state.\nb. Less Starvation\nc. High CPU utilization","subsubtopics":[]},{"name":"Different time","explanation":"1. Throughput: No. of processes completed per unit time.\n2. Arrival time (AT): Time when process is arrived at the ready queue.\n3. Burst time (BT): The time required by the process for its execution.\n4. Turnaround time (TAT): Time taken from first time process enters ready state till it terminates. (CT - AT)\n5. Wait time (WT): Time process spends waiting for CPU. (WT = TAT – BT)\n6. Response time: Time duration between process getting into ready queue and process getting CPU for the\nfirst time.\n7. Completion Time (CT): Time taken till process gets terminated.","subsubtopics":[]},{"name":"Goals","explanation":"1. Max CPU utilization\n2. Minimum turn around time(time from ready queue to terminate)\n3. Minimum wait time. (time in ready queue)\n4. Minimum response time. (time from ready queue to 1st CPU time)\n5. Maximum throughput (No. of process complete per unit time.)\n","subsubtopics":[]},{"name":"FCFS","explanation":"a. Whichever process comes first in the ready queue will be given CPU first.\nb. In this, if one process has longer BT. It will have major effect on average WT of diff processes, called convoy effect.","subsubtopics":[]},{"name":"Shortest Job First (SJF) [Non-preemptive]","explanation":"a. Process with least BT will be dispatched to CPU first.\nb. Must do estimation for BT for each process in ready queue beforehand, Correct estimation of BT is an impossible task (ideally.)\nc. Run lowest time process for all time then, choose job having lowest BT at that instance.\nd. This will suffer from convoy effect as if the very first process which came is Ready state is having a\nlarge BT.\ne. Process starvation might happen.\nf. Criteria for SJF algos, AT + BT","subsubtopics":[]},{"name":"SJF [Preemptive]\n","explanation":"a. Less starvation.\nb. No convoy effect.\nc. Gives average WT less for a given set of processes as scheduling short job before a long one decreases the WT of short job more than it increases the WT of the long process.\n\n","subsubtopics":[]},{"name":"Priority Scheduling [Non-preemptive]","explanation":"a. Priority is assigned to a process when it is created.\nb. SJF is a special case of general priority scheduling with priority inversely proportional to BT.\n","subsubtopics":[]},{"name":"Priority Scheduling [Preemptive]","explanation":"a. Current RUN state job will be preempted if next job has higher priority.\n\nb. May cause indefinite waiting (Starvation) for lower priority jobs. (Possibility is they won’t get executed ever). (True for both preemptive and non-preemptive version)\ni. Solution: Ageing is the solution.\nii. Gradually increase priority of process that wait so long. E.g., increase priority by 1 every 15 minutes.","subsubtopics":[]},{"name":"Round robin scheduling (RR)","explanation":"a. Most popular\nb. Like FCFS but preemptive.\nc. Designed for time sharing systems.\nd. Criteria: AT + time quantum (TQ), Doesn’t depend on BT.\ne. No process is going to wait forever, hence very low starvation. [No convoy effect]\nf. Easy to implement.\ng. If TQ is small, more will be the context switch (more overhead).","subsubtopics":[]},{"name":"Multi-level queue scheduling (MLQ)","explanation":"Here we are dividing the processes into 3 ready queue.\n1. system process\n2. Interactive process (I/O type process)\n3. Background Process\n\nWe have 3 ready queue named as \nSP queue, IP queue, BP queue.\n\nEach queue follow its own Algo.\nSP -> RR, IP -> RR & BP -> FCFS\n\npriority of SP> IP> BP.\n\nNOTE: A process is permanently assigned to one of the queues (inflexible) based on some property of process, memory, size, process priority or process type\n\n\nScheduling among different sub-queues is implemented as fixed priority preemptive scheduling. E.g., foreground queue has absolute priority over background queue.\n\nIf an interactive process comes & batch process is currently executing. Then, batch process will be preempted.\n\nProblem: Only after completion of all the processes from the top-level ready queue, the further level ready queues will be scheduled. \nThis came starvation for lower priority process.\n\nConvoy effect is present","subsubtopics":[]},{"name":"Multi-level feedback queue scheduling (MLFQ)","explanation":"Note: Here the process can move between queues. The idea behind is to separate processes according to the characteristics of their BT. If a process uses too much CPU time, it will be moved to lower priority queue. This scheme leaves I/O bound and interactive processes in the higher-priority queue.\n\nIn addition, a process that waits too much in a lower-priority queue may be moved to a higher priority queue. This form of ageing prevents starvation.\n\nLess starvation then MLQ.\n\nIt is flexible.\n\nCan be configured to match a specific system design requirement.\n","subsubtopics":[]},{"name":"Process starvation and convoy effect","explanation":"Process Starvation: Process starvation occurs when a process is unable to proceed because it is waiting for a resource that is being continuously allocated to other processes. The process remains in a blocked state indefinitely, unable to make progress. This can happen if a scheduling algorithm favors certain processes over others, leading to some processes being starved of resources.\n\nConvoy Effect: The convoy effect, also known as the convoy phenomenon, occurs when multiple processes are forced to wait for a single shared resource, even though they may not be directly dependent on each other. This can lead to inefficiency and decreased system performance. The name \"convoy effect\" is derived from the analogy of a slow-moving convoy of vehicles, where faster vehicles are forced to slow down and follow the pace of the slowest vehicle.\n\nIn summary, process starvation is about a single process being denied resources it needs, while the convoy effect is about multiple processes being unnecessarily delayed due to contention for a shared resource. Both can lead to inefficiencies in system performance, but they arise from different scenarios and causes.","subsubtopics":[]}]},{"name":"Concurrency","explanation":"Theard All details\nConcurrency definition\ncritical section\nrace condition\nSolution to race condition\nSemaphores\nAtomic Operation\nMutual Exclusion\nStore a bool variable\nPeterson's solution","images":["http://localhost:4000/uploads/1718199132279.png"],"subtopics":[{"name":"Thread","explanation":"Thread Scheduling and process scheduling (Done using CPU scheduling algos)\n4. Threads context switching\n5. How each thread get access to the CPU?\n6. I/O or TQ, based context switching is done here as well\n7. Will single CPU system would gain by multi-threading technique?\n8. Benefits of Multi-threading.\n","subsubtopics":[{"name":"Explain","explanation":"Single sequence stream within a process.\n• An independent path of execution in a process.\n• Light-weight process.\n• Used to achieve parallelism by dividing a process’s tasks which are independent path of\nexecution.\n• E.g., Multiple tabs in a browser, text editor (When you are typing in an editor, spell\nchecking, formatting of text and saving the text are done concurrently by multiple threads.)"},{"name":"Thread Scheduling and process scheduling \n(Done using CPU scheduling algos)","explanation":"Process Scheduling:\n\nThis term is traditionally used to describe the scheduling of entire processes.\nEach process is considered a single unit of execution.\nProcesses have separate memory spaces and resources.\n\n\nThread Scheduling:\n\nIn multi-threaded environments, the term \"thread scheduling\" is more accurate.\nThreads within processes are scheduled independently.\nThreads share the same memory space and resources within their parent process."},{"name":"Thread scheduling summary","explanation":"Ready Queue:\n\nContains individual threads from all processes that are ready to run.\nEach thread is treated as an independent unit of execution.\nCPU Scheduling:\n\nThe CPU scheduling algorithm selects threads from the ready queue.\nThe selected thread is assigned CPU time and allowed to execute.\nAfter the thread's time slice expires or it gets blocked, the scheduler picks another thread from the ready queue.\nExample Scenario\nConsider a system with three processes (P1, P2, P3), each having multiple threads:\n\nProcesses and Threads:\n\nP1 has threads T1, T2.\nP2 has thread T3.\nP3 has threads T4, T5.\nReady Queue:\n\nThe ready queue might look like this: [T1, T2, T3, T4, T5].\nScheduling Process:\n\nThe scheduler picks T1, assigns it CPU time, and T1 runs.\nWhen T1's time slice ends, the scheduler picks the next thread (e.g., T3), and T3 runs.\nThis continues with T2, T4, T5, and so on, based on the scheduling policy."},{"name":"4. Threads context switching\n","explanation":"OS saves current state of thread & switches to another thread of same process.\n• Doesn’t includes switching of memory address space. (But Program counter, registers &\nstack are included.)\n• Fast switching as compared to process switching\n• CPU’s cache state is preserved"},{"name":"5. How each thread get access to the CPU?\n","explanation":"Each thread has its own program counter.\n• Depending upon the thread scheduling algorithm, OS schedule these threads.\n• OS will fetch instructions corresponding to PC of that thread and execute instruction"},{"name":"6. I/O or TQ, based context switching is done here as well\n","explanation":"We have TCB (Thread control block) like PCB for state storage management while\nperforming context switching."},{"name":"Will single CPU system would gain by multi-threading technique?","explanation":"Never.\n• As two threads have to context switch for that single CPU.\n• This won’t give any gain"},{"name":"8. Benefits of Multi-threading.\n","explanation":"Responsiveness\n• Resource sharing: Efficient resource sharing.\n• Economy: It is more economical to create and context switch threads.\n1. Also, allocating memory and resources for process creation is costly, so better to\ndivide tasks into threads of same process.\n• Threads allow utilization of multiprocessor architectures to a greater scale and efficiency"},{"name":"sub-sub-topic_name","explanation":""}]},{"name":"What is concurrency?","explanation":"Simple words : Ability of OS to do multiple tasks.\n\nexplaination:\n\nConcurrency in an operating system (OS) refers to its ability to manage and execute multiple tasks or processes  at the same time. This creates the illusion of simultaneous execution even on single-core processors. Here's a breakdown of the concept:\n\nCore Idea:  Imagine you're multitasking - writing an email while downloading a file. Concurrency in OS allows the OS to handle these tasks concurrently, even if your CPU can only focus on one task at a time.","subsubtopics":[]},{"name":"Cricical section and Race Condition","explanation":"Imagine a bank account with a balance stored in a variable balance. Two people (represented by threads) can try to withdraw money at the same time:\n","subsubtopics":[{"name":"Critical Section:","explanation":"The critical section is the part of the code that modifies the shared resource (balance) in this case. It involves:\n\nRead balance: Get the current balance from the variable.\nUpdate balance: Subtract the withdrawal amount from the balance.\nWrite balance: Store the new balance back to the variable.\n"},{"name":"Race Condition:\n","explanation":"A race condition occurs when the outcome depends on the unpredictable timing of thread execution. Here's how it can happen:\n\nThread A enters: Thread A reads the current balance (say, $100).\nThread B enters (Race!): Before Thread A finishes updating, Thread B also reads the same balance ($100).\nThread A updates: Thread A subtracts its withdrawal amount (say, $50) and gets $50.\nThread B updates: Thread B subtracts its withdrawal amount (say, $30) from the old value read earlier ($100), resulting in $70.\nThread A writes: Thread A writes its new balance ($50) to the variable.\nThread B writes: Thread B writes its new balance ($70), overwriting the correct value from Thread A.\nResult: The final balance becomes $70, which is incorrect because it doesn't reflect the withdrawal made by Thread A. This is a race condition because the final outcome depends on the timing of thread execution.\n\n\n\n\nA race condition occurs when two or more threads can access shared\ndata and they try to change it at the same time. Because the thread\nscheduling algorithm can swap between threads at any time, you\ndon't know the order in which the threads will attempt to access the\nshared data. Therefore, the result of the change in data is dependent\non the thread scheduling algorithm, i.e., both threads are \"racing\" to\naccess/change the data"}]},{"name":"Solution to race condition","explanation":"we want a solution that will\n\n1. remove race condition\n2. Progress ( there is no directional order that if T1 out then it should not like only T2 has to go or if the critical section is free then there should any thread can go ).\n3. No one thread gets indefinate waiting. there should be time bound that after that time thread must get the CS. (not mandatory)\n\n\nSemaphores\nAtomic Operation\nMutual Exclusion\nStore a bool variable\nPeterson's solution","subsubtopics":[{"name":"Atomic Operation","explanation":" Make Critical code section an atomic operation, i.e.,\nExecuted in one CPU cycle.\nonly Some languages gives us this feature."},{"name":"Mutual Exclusion","explanation":"It means somehow do that whenever T1 is doing operation on critical section then do not allow any other thread to work on.\n\nAll Language provides us locks/mutax. It will lock the critical section.\nsee file in OS folder "},{"name":"Semaphores","explanation":""},{"name":"Store a bool variable","explanation":"Bool value has initially false value.\nIt tells that if both threads are not using the CS. But only T1 has access to CS first.\n\nHence, It is not a good solution"},{"name":"peterson's solution","explanation":"It is useful for only 2 threads\nBut why?\n\nShared Variables:\n\nflag[0] and flag[1]: Boolean flags (one for each process) indicating if a process wants to enter the critical section.\nturn: An integer variable that remembers whose turn it is to enter (0 or 1).\nProcess Steps:\n\nWant to Enter: Set your flag (flag[i]) to true, indicating you want to enter the critical section.\nCheck Other Process: See if the other process's flag (flag[j]) is also true.\nYield Turn (if needed): If the other process wants in (flag is true) and it's their turn (based on turn), you defer by setting turn to j (other process's index).\nBusy Waiting: Keep checking both flags (flag[0] and flag[1]) until it's safe to enter (both are false or other process deferred).\nCritical Section: Enter your critical section code that accesses the shared resource.\nExit: Set your flag back to false to indicate you're done and reset turn to the other process (fairness).\nThis ensures only one process enters the critical section at a time, preventing race conditions."}]},{"name":"Mutual Exclusion","explanation":"Locks can be used to implement mutual exclusion and avoid race condition\nby allowing only one thread/process to access critical section.","subsubtopics":[{"name":"Dis-advantage","explanation":"1. Contention: one thread has acquired the lock, other threads will be\nbusy waiting, what if thread that had acquired the lock dies, then all\nother threads will be in infinite waiting.\n\n2. Deadlocks\n\n3. Debugging problem\n\n4. Starvation of high priority threads. ( let low priority thread is using and suddenly high priority thread comes it will do context switching and due to which in the mid way low priority thread stops run and the resoures is still locked)"}]},{"name":"Conditional variables","explanation":"\nA condition variable is a synchronization primitive that allows threads to wait until a particular condition occurs. Condition variables are used in conjunction with a mutex (lock) to allow threads to wait for some condition to be met before continuing execution. They are typically used in scenarios where a thread needs to wait for a specific state or condition to occur before it can proceed.\n\nSure, let's focus on understanding the Condition object and how to use its methods such as wait() and notify_all() for thread synchronization. Here's a simple example to illustrate how a Condition can be used to coordinate between threads.\n\nUnderstanding Condition\nA Condition object allows one or more threads to wait until they are notified. It is often used in conjunction with a shared resource that threads need to access or modify. The Condition object uses an underlying lock to ensure that only one thread can access the shared resource at a time.\n\nKey Methods\nacquire() and release(): These methods are used to manually acquire and release the underlying lock.\nwait(): This method releases the lock and puts the thread into a wait state until it is notified.\nnotify(): This method wakes up one of the waiting threads.\nnotify_all(): This method wakes up all the waiting threads.\n\n.\nd. Why to use conditional variable?\ni. To avoid busy waiting.\ne. Contention is not here\n\n\nsee hands on example in the code ","subsubtopics":[{"name":"Detailed example","explanation":"Yes, the statement `with cond:` is used to acquire the lock associated with the condition variable `cond`. In Python, a `Condition` object is a combination of a lock (mutex) and a condition variable. When you use `with cond:`, it means the thread is acquiring the lock associated with `cond`.\n\nHere is a detailed breakdown of the code and the usage of the condition variable:\n\n```python\nfrom threading import *\nimport time\n\n# Create a condition variable object\ncond = Condition() \ndone = 1 \n\ndef task(name):\n    global done\n    with cond:  # Acquire the condition variable's associated lock\n        if done == 1:\n            done = 2  # Change the state to indicate a task is waiting\n            print(\"waiting on condition variable cond:\", name)\n            cond.wait()  # Release the lock and wait for the condition to be notified\n            print(\"Condition met: \", name) \n        else:\n            for i in range(5):\n                print(\".\")\n                time.sleep(1)  # Simulate work with a sleep\n\n            print(\"signaling condition variable cond\", name)\n            cond.notify_all()  # Notify all waiting threads\n            print(\"Notification done\", name)\n\nif __name__ == '__main__':\n    # Create two threads that run the task function\n    t1 = Thread(target=task, args=('t1',)) \n    t2 = Thread(target=task, args=('t2',)) \n\n    # Start the threads\n    t1.start() \n    t2.start() \n\n    # Wait for both threads to complete\n    t1.join()\n    t2.join()\n```\n\n### Step-by-Step Execution with Comments:\n\n1. **Initialization**:\n   - A `Condition` object `cond` and a global variable `done` are initialized.\n\n2. **Thread Creation and Starting**:\n   - Two threads, `t1` and `t2`, are created with the `task` function as their target.\n   - Both threads are started almost simultaneously.\n\n3. **Thread `t1` Execution**:\n   - **Acquire Lock**: `t1` starts and reaches the `with cond:` statement, which acquires the lock associated with the condition variable `cond`. This ensures that no other thread can execute the critical section of code protected by this lock simultaneously.\n   - **Check Condition**: `t1` checks `if done == 1:`, which is true initially.\n   - **Modify Shared State**: `t1` sets `done` to 2 and prints \"waiting on condition variable cond: t1\".\n   - **Wait on Condition**: `t1` calls `cond.wait()`. This action:\n     - Releases the lock, allowing other threads to acquire it.\n     - Puts `t1` into a waiting state, blocking it until it is notified.\n     - This effectively means `t1` stops executing here and will only continue when it is notified by another thread.\n\n4. **Thread `t2` Execution**:\n   - **Acquire Lock**: `t2` starts and reaches the `with cond:` statement, which acquires the lock associated with the condition variable `cond`. Since `t1` has released the lock by calling `cond.wait()`, `t2` can acquire it.\n   - **Check Condition**: `t2` checks `if done == 1:`, which is now false since `done` was set to 2 by `t1`.\n   - **Execute Else Block**: `t2` enters the `else` block and prints dots with a 1-second sleep in between (`for i in range(5):`).\n   - **Notify Condition**: After the loop, `t2` prints \"signaling condition variable cond t2\", calls `cond.notify_all()`, which:\n     - Wakes up `t1` from its waiting state.\n     - `t1` re-acquires the lock before continuing.\n   - **Print Notification Done**: `t2` prints \"Notification done t2\".\n\n5. **Resumption of `t1`**:\n   - **Wake Up and Re-acquire Lock**: `t1` wakes up from the wait because of the `notify_all()` call from `t2`, re-acquires the lock associated with `cond`, and continues executing the code following the `cond.wait()` call.\n   - **Print Condition Met**: `t1` prints \"Condition met: t1\" and completes execution.\n\n6. **Main Thread**:\n   - The main thread waits for both `t1` and `t2` to complete by calling `t1.join()` and `t2.join()`.\n\n### Summary\n\n- **`with cond:`**: This acquires the lock associated with the condition variable `cond`. This lock ensures that only one thread can execute the critical section of code protected by this lock at a time.\n- **`cond.wait()`**: This releases the lock and puts the calling thread into a waiting state until it is notified. Means the thread is calling itself that now we start waiting and let the other threads starts work.\nBecause t1 has released the lock (by calling cond.wait()), t2 can now acquire the lock using with cond:.\n- **`cond.notify_all()`**: This wakes up all threads waiting on the condition variable. The waiting threads will then attempt to re-acquire the lock and continue executing.\n\nThe lock acquisition and release mechanism ensures proper synchronization between threads, allowing them to wait for and signal conditions safely."},{"name":"Explaination of conditional variable 2.py ","explanation":"Explanation\nCondition Object Creation:\n\ncondition = threading.Condition() creates a Condition object.\nProducer Function:\n\nThe producer acquires the condition lock using with condition.\nIt prints a message indicating it is producing an item.\nIt adds an item to the shared_resource list.\nIt notifies one waiting thread using condition.notify().\nConsumer Function:\n\nThe consumer acquires the condition lock using with condition.\nIt enters a while loop checking if the shared_resource list is empty.\nIf the list is empty, it prints a message indicating it is waiting and then calls condition.wait(), releasing the lock and waiting to be notified.\nWhen notified, it resumes execution, acquires the lock, and consumes an item from the shared_resource list.\nThread Creation and Execution:\n\nThe main thread creates and starts a consumer thread and a producer thread.\nThe consumer thread starts first and enters the waiting state.\nThe producer thread starts after a short delay, adds an item, and notifies the waiting consumer.\nBoth threads are joined, ensuring the main thread waits for their completion before exiting.\nThis example demonstrates how to use Condition to synchronize the actions of a producer and a consumer, ensuring that the consumer waits for an item to be produced before consuming it."}]},{"name":"Semaphores","explanation":"A semaphore is a synchronization primitive used in concurrent programming to control access to a common resource by multiple threads and to avoid critical section problems. It is a variable or abstract data type that provides a simple but powerful mechanism for regulating traffic patterns in a concurrent system.\n\nTypes of Semaphores\nCounting Semaphore: This type of semaphore can have a non-negative integer value and is used to control access to a resource that has a limited number of instances. The value of the semaphore represents the number of available resources.\n\nBinary Semaphore (or Mutex): This is a special case of a counting semaphore that can only take the values 0 and 1. It is used to ensure mutual exclusion, allowing only one thread to access a critical section at a time.\n\nSemaphore Operations\nSemaphores provide two atomic operations, often referred to as P (wait) and V (signal) operations:\n\nP (wait, down, or acquire):\nDecreases the semaphore value by 1.\nIf the semaphore value is less than 0, the process is blocked until the value becomes greater than or equal to 0.\nV (signal, up, or release):\nIncreases the semaphore value by 1.\nIf there are processes waiting (i.e., the value is less than or equal to 0), one of the waiting processes is unblocked.\n\n\nsee the code in the OS folder","subsubtopics":[{"name":"Code explaintation","explanation":"Detailed Explanation\nThread Creation and Start:\n\nThe main thread creates five threads (Thread 1 to Thread 5), each targeting the task function.\nEach thread is started, causing them to execute the task function concurrently.\nThreads Enter Waiting State:\n\nEach thread acquires the condition lock using with condition.\nEach thread prints that it is waiting for the condition.\nEach thread then calls condition.wait(), which releases the condition lock and puts the thread into a waiting state.\nMain Thread Notification:\n\nThe main thread sleeps for a short duration (time.sleep(1)) to ensure that all threads are in the waiting state.\nThe main thread then acquires the condition lock using with condition and calls condition.notify_all(). This wakes up all the threads that are waiting on the condition variable.\nThreads Continue Execution:\n\nUpon being notified, each thread exits the wait state, reacquires the condition lock, and then releases it by exiting the with condition block.\nEach thread prints that it is trying to acquire the semaphore.\nEach thread attempts to acquire the semaphore using with sem. Up to three threads can acquire the semaphore simultaneously.\nEach thread that acquires the semaphore prints that it has acquired the semaphore, simulates some work by sleeping for 2 seconds, and then releases the semaphore.\nCondition Notification by Each Thread:\n\nAfter releasing the semaphore, each thread reacquires the condition lock using with condition and calls condition.notify_all(). This ensures that if any other threads are waiting on the condition variable, they are notified and can proceed.\nEach thread then exits the task function.\nMain Thread Waits for Completion:\n\nThe main thread uses a loop to join all threads, ensuring that it waits for each thread to complete its execution before exiting.\nKey Points\nThe initial condition.notify_all() is called by the main thread to wake up all the threads that are initially waiting.\nEach thread, after completing its work and releasing the semaphore, calls condition.notify_all() again to ensure that any threads that go into a waiting state during the execution can be notified and proceed.\nThe condition.wait() method releases the condition lock and puts the thread into a waiting state until it is notified.\nThis flow ensures that the main thread can control when all threads start working, and each thread can notify others upon completing its task to maintain smooth coordination."}]}]},{"name":"Interview Questions","explanation":"","images":[],"subtopics":[{"name":"Producer Consumer Problem(lect 18)","explanation":"","subsubtopics":[]},{"name":"Reader-writer Problem (lect 19)","explanation":"","subsubtopics":[]},{"name":"The Dining Philosophers Problem(lect 20)","explanation":"","subsubtopics":[]}]},{"name":"DeadLock","explanation":"1. In Multi-programming environment, we have several processes competing for finite number of\nresources\n2. Process requests a resource (R), if R is not available (taken by other process), process enters in a\nwaiting state. Sometimes that waiting process is never able to change its state because the resource,\nit has requested is busy (forever), called DEADLOCK (DL)\n3. Two or more processes are waiting on some resource’s availability, which will never be available as\nit is also busy with some other process. The Processes are said to be in Deadlock.\n4. DL is a bug present in the process/thread synchronization method.\n5. In DL, processes never finish executing, and the system resources are tied up, preventing other jobs\nfrom starting.\n6. Example of resources: Memory space, CPU cycles, files, locks, sockets, IO devices etc.\n7. Single resource can have multiple instances of that. E.g., CPU is a resource, and a system can have 2\nCPUs\n\n\nHow a process/thread utilize a resource?\nDeadlock Necessary Condition\nMethods for handling Deadlocks:\na. deadlock Prevention\nDeadlock avoidance\nOstrick Algo\nBanker Algos\nDeadlock detection\nRecovery from Deadlock\n","images":[],"subtopics":[{"name":"How a process/thread utilize a resource?","explanation":"a. Request: Request the R, if R is free Lock it, else wait till it is available.\nb. Use\nc. Release: Release resource instance and make it available for other processes","subsubtopics":[]},{"name":"Deadlock Necessary Condition","explanation":"4 Condition should hold simultaneously.\n\n\na. Mutual Exclusion\ni. Only 1 process at a time can use the resource, if another process requests that\nresource, the requesting process must wait until the resource has been released.\nb. Hold & Wait\ni. A process must be holding at least one resource & waiting to acquire additional\nresources that are currently being held by other processes.\nc. No-preemption\ni. Resource must be voluntarily released by the process after completion of\nexecution. (No resource preemption)\nd. Circular wait\ni. A set {P0, P1, … ,Pn} of waiting processes must exist such that P0 is waiting for a\nresource held by P1, P1 is waiting for a resource held by P2, and so on.","subsubtopics":[]},{"name":"Methods for handling Deadlocks:","explanation":"a. Use a protocol to prevent or avoid deadlocks, ensuring that the system will never enter a deadlocked state.\nb. Allow the system to enter a deadlocked state, detect it, and recover.\nc. Ignore the problem altogether and pretend that deadlocks never occur in system. (Ostrich\nalgorithm) aka, Deadlock ignorance.","subsubtopics":[]},{"name":"a. deadlock Prevention","explanation":" by ensuring at least one of the necessary conditions cannot hold","subsubtopics":[{"name":"a. Mutual exclusion\n","explanation":"i. Use locks (mutual exclusion) only for non-sharable resource.\nii. Sharable resources like Read-Only files can be accessed by multiple\nprocesses/threads.\niii. However, we can’t prevent DLs by denying the mutual-exclusion condition,\nbecause some resources are intrinsically non-sharable"},{"name":"b. Hold & Wait\n","explanation":"i. To ensure H&W condition never occurs in the system, we must guarantee that,\nwhenever a process requests a resource, it doesn’t hold any other resource.\nii. Protocol (A) can be, each process has to request and be allocated all its resources\nbefore its execution.\niii. Protocol (B) can be, allow a process to request resources only when it has none. It\ncan request any additional resources after it must have released all the resources\nthat it is currently allocated."},{"name":"c. No preemption\n","explanation":"i. If a process is holding some resources and request another resource that cannot\nbe immediately allocated to it, then all the resources the process is currently\nholding are preempted. The process will restart only when it can regain its old\nresources, as well as the new one that it is requesting. (Live Lock may occur).\nii. If a process requests some resources, we first check whether they are available. If\nyes, we allocate them. If not, we check whether they are allocated to some other\nprocess that is waiting for additional resources. If so, preempt the desired resource\nfrom waiting process and allocate them to the requesting process"},{"name":"d. Circular wait\n","explanation":"i.. To ensure that this condition never holds is to impose a proper ordering of\nresource allocation.\nii. P1 and P2 both require R1 and R1, locking on these resources should be like, both\ntry to lock R1 then R2. By this way which ever process first locks R1 will get R2"}]},{"name":"Deadlock avoidance","explanation":": Idea is, the kernel be given in advance info concerning which resources will\nuse in its lifetime.\nBy this, system can decide for each request whether the process should wait.\nTo decide whether the current request can be satisfied or delayed, the system must consider the\nresources currently available, resources currently allocated to each process in the system and the\nfuture requests and releases of each process.\na. Schedule process and its resources allocation in such a way that the DL never occur.\nb. Safe state: A state is safe if the system can allocate resources to each process (up to its\nmax.) in some order and still avoid DL.\nA system is in safe state only if there exists a safe sequence.\nc. In an Unsafe state, the operating system cannot prevent processes from requesting\nresources in such a way that any deadlock occurs. It is not necessary that all unsafe states\nare deadlocks; an unsafe state may lead to a deadlock.\nd. The main key of the deadlock avoidance method is whenever the request is made for\nresources then the request must only be approved only in the case if the resulting state is a\nsafe state.\ne. In a case, if the system is unable to fulfill the request of all processes, then the state of the\nsystem is called unsafe.\nf. Scheduling algorithm using which DL can be avoided by finding safe state. (Banker\nAlgorithm)","subsubtopics":[]},{"name":"Ostrick Algo","explanation":"Told application programmer to write a code that can't lead to deadlock","subsubtopics":[]},{"name":"Banker Algorithm","explanation":"When a process requests a set of resources, the system must determine whether allocating\nthese resources will leave the system in a safe state. If yes, then the resources may be\nallocated to the process. If not, then the process must wait till other processes release\nenough resources","subsubtopics":[]},{"name":"Deadlock Detection","explanation":"ystems haven’t implemented deadlock-prevention or a deadlock avoidance\ntechnique, then they may employ DL detection then, recovery technique.\na. Single Instance of Each resource type (wait-for graph method)\ni. A deadlock exists in the system if and only if there is a cycle in the wait-for graph.\nIn order to detect the deadlock, the system needs to maintain the wait-for graph\nand periodically system invokes an algorithm that searches for the cycle in the\nwait-for graph.\nb. Multiple instances for each resource type\ni. Banker Algorithm","subsubtopics":[]},{"name":"Recovery from Deadlock","explanation":"a. Process termination\ni. Abort all DL processes\nii. Abort one process at a time until DL cycle is eliminated.\nb. Resource preemption\ni. To eliminate DL, we successively preempt some resources from processes and\ngive these resources to other processes until DL cycle is broken.","subsubtopics":[]}]},{"name":"Memory Management","explanation":"In Multi-programming environment, we have multiple processes in the main memory (Ready Queue) to\nkeep the CPU utilization high and to make computer responsive to the users.\n2. To realize this increase in performance, however, we must keep several processes in the memory; that is, we\nmust share the main memory. As a result, we must manage main memory for all the different processes.","images":["http://localhost:4000/uploads/1718260886448.png","http://localhost:4000/uploads/1718260892938.png","http://localhost:4000/uploads/1718263711470.png","http://localhost:4000/uploads/1718263718438.png","http://localhost:4000/uploads/1718263726073.png"],"subtopics":[{"name":"Logical versus Physical Address Space\n","explanation":"The runtime mapping from virtual to physical address is done by a hardware device called the\nmemory-management unit (MMU).\n\n The user's program mainly generates the logical address, and the user thinks that the program is\nrunning in this logical address, but the program mainly needs physical memory in order to\ncomplete its execution.","subsubtopics":[{"name":"a. Logical Address\n","explanation":"i. An address generated by the CPU.\nii. The logical address is basically the address of an instruction or data used by a process.\niii. User can access logical address of the process.\niv. User has indirect access to the physical address through logical address.\nv. Logical address does not exist physically. Hence, aka, Virtual address.\nvi. The set of all logical addresses that are generated by any program is referred to as Logical\nAddress Space.\nvii. Range: 0 to max."},{"name":"b. Physical Address\n","explanation":"i. An address loaded into the memory-address register of the physical memory.\nii. User can never access the physical address of the Program.\niii. The physical address is in the memory unit. It’s a location in the main memory physically.\niv. A physical address can be accessed by a user indirectly but not directly.\nv. The set of all physical addresses corresponding to the Logical addresses is commonly\nknown as Physical Address Space.\nvi. It is computed by the Memory Management Unit (MMU).\nvii. Range: (R + 0) to (R + max), for a base value R."}]},{"name":"How OS manages the isolation and protect? (Memory Mapping and Protection)","explanation":"a. OS provides this Virtual Address Space (VAS) concept.\nb. To separate memory space, we need the ability to determine the range of legal addresses that the\nprocess may access and to ensure that the process can access only these legal addresses.\nc. The relocation register contains value of smallest physical address (Base address [R]); the limit\nregister contains the range of logical addresses (e.g., relocation = 100040 & limit = 74600).\nd. Each logical address must be less than the limit register.\ne. MMU maps the logical address dynamically by adding the value in the relocation register.\nf. When CPU scheduler selects a process for execution, the dispatcher loads the relocation and limit \nregisters with the correct values as part of the context switch. Since every address generated by the \nCPU (Logical address) is checked against these registers, we can protect both OS and other users’ \nprograms and data from being modified by running process.\ng. Any attempt by a program executing in user mode to access the OS memory or other uses’ \nmemory results in a trap in the OS, which treat the attempt as a fatal error.\nh. Address Translation (fig 2)","subsubtopics":[]},{"name":"5. Allocation Method on Physical Memory\n","explanation":"a. Contiguous Allocation\nb. Non-contiguous Allocation","subsubtopics":[]},{"name":"6. Contiguous Memory Allocation\n","explanation":"a. In this scheme, each process is contained in a single contiguous block of memory.\nb. Fixed Partitioning\ni. The main memory is divided into partitions of equal or different sizes\n\nc. Dynamic Partitioning\n","subsubtopics":[{"name":"Limitations","explanation":"1. Internal Fragmentation: if the size of the process is lesser then the total size of \nthe partition then some size of the partition gets wasted and remain unused. \nThis is wastage of the memory and called internal fragmentation.\n2. External Fragmentation: The total unused space of various partitions cannot be \nused to load the processes even though there is space available but not in the \ncontiguous form.\n3. Limitation on process size: If the process size is larger than the size of maximum \nsized partition then that process cannot be loaded into the memory. Therefore, a \nlimitation can be imposed on the process size that is it cannot be larger than the \nsize of the largest partition.\n4. Low degree of multi-programming: In fixed partitioning, the degree of \nmultiprogramming is fixed and very less because the size of the partition cannot \nbe varied according to the size of processes."},{"name":"c. Dynamic Partitioning\n","explanation":"1. In this technique, the partition size is not declared initially. It is declared at the time of \nprocess loading.\nii. Advantages over fixed partitioning\n1.. No internal fragmentation\n2. No limit on size of process\n3. Better degree of multi-programming\n\nLimitation : external fragmentation"}]},{"name":" Free Space Management","explanation":"","subsubtopics":[{"name":"Defragmentation/Compaction","explanation":"a. Dynamic partitioning suffers from external fragmentation.\nb. Compaction to minimize the probability of external fragmentation.\nc. All the free partitions are made contiguous, and all the loaded partitions are brought together.\nd. By applying this technique, we can store the bigger processes in the memory. The free partitions\nare merged which can now be allocated according to the needs of new processes. This technique is\nalso called defragmentation.\ne. The efficiency of the system is decreased in the case of compaction since all the free spaces will be\ntransferred from several places to a single place."},{"name":"2. How free space is stored/represented in OS?\n","explanation":"a. Free holes in the memory are represented by a free list (Linked-List data structure).\n"},{"name":"3. How to satisfy a request of a of n size from a list of free holes?\n","explanation":"a. Various algorithms which are implemented by the Operating System in order to find out the holes\nin the linked list and allocate them to the processes.\nb. First Fit\ni. Allocate the first hole that is big enough.\nii. Simple and easy to implement.\niii. Fast/Less time complexity\nc. Next Fit\ni. Enhancement on First fit but starts search always from last allocated hole.\nii. Same advantages of First Fit.\nd. Best Fit\ni. Allocate smallest hole that is big enough.\nii. Lesser internal fragmentation.\niii. May create many small holes and cause major external fragmentation.\niv. Slow, as required to iterate whole free holes list.\ne. Worst Fit\ni. Allocate the largest hole that is big enough.\nii. Slow, as required to iterate whole free holes list.\niii. Leaves larger holes that may accommodate other processes"}]}]},{"name":"Memory Management 2","explanation":"","images":["http://localhost:4000/uploads/1718264305047.jpeg","http://localhost:4000/uploads/1718264311756.jpeg","http://localhost:4000/uploads/1718264317443.jpeg","http://localhost:4000/uploads/1718264323249.jpeg","http://localhost:4000/uploads/1718264328274.jpeg"],"subtopics":[]},{"name":"Storage Management and security","explanation":"","images":[],"subtopics":[]}]}}